{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447880ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Hide all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea273d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d08097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three raw datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#   Load the three raw datasets  \n",
    "try:\n",
    "    df_employee = pd.read_csv('data/employee_data.csv')\n",
    "    df_insurance = pd.read_csv('data/insurance_data.csv')\n",
    "    df_vendor = pd.read_csv('data/vendor_data.csv')\n",
    "    print(\"All three raw datasets loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading raw datasets: {e}\")\n",
    "    print(\"Please ensure 'employee_data.csv', 'insurance_data.csv', and 'vendor_data.csv' are in a 'data' subdirectory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5cd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2306aed",
   "metadata": {},
   "source": [
    " Initial Cleaning and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408d850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cleaning and merging complete.\n",
      "Merged DataFrame shape: (10000, 54)\n",
      "'data/merged_insurance_data.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "#   Perform initial cleaning and merging from EDA phase  \n",
    "\n",
    "# 1. Convert Date Columns\n",
    "date_cols_insurance = ['TXN_DATE_TIME', 'POLICY_EFF_DT', 'LOSS_DT', 'REPORT_DT']\n",
    "for col in date_cols_insurance:\n",
    "    df_insurance[col] = pd.to_datetime(df_insurance[col], errors='coerce')\n",
    "\n",
    "df_employee['DATE_OF_JOINING'] = pd.to_datetime(df_employee['DATE_OF_JOINING'], errors='coerce')\n",
    "\n",
    "# 2. Merge DataFrames\n",
    "df_merged = pd.merge(df_insurance, df_employee, on='AGENT_ID', how='left', suffixes=('', '_emp'))\n",
    "df_merged = pd.merge(df_merged, df_vendor, on='VENDOR_ID', how='left', suffixes=('', '_vend'))\n",
    "\n",
    "# 3. Create Target Variable (FRAUD_FLAG)\n",
    "status_to_fraud_map = {'A': 0, 'D': 1} # A=Approved (Legit), D=Denied (Fraud)\n",
    "df_merged['FRAUD_FLAG'] = df_merged['CLAIM_STATUS'].map(status_to_fraud_map)\n",
    "\n",
    "# 4. Save the merged file (this resolves the error)\n",
    "df_merged.to_csv('data/merged_insurance_data.csv', index=False)\n",
    "\n",
    "print(\"Initial cleaning and merging complete.\")\n",
    "print(\"Merged DataFrame shape:\", df_merged.shape)\n",
    "print(\"'data/merged_insurance_data.csv' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72703297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "631626da",
   "metadata": {},
   "source": [
    "Load the Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c88950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'merged_insurance_data.csv' loaded successfully for feature engineering.\n",
      "Shape: (10000, 54)\n"
     ]
    }
   ],
   "source": [
    "#   Load the newly created merged dataset  \n",
    "try:\n",
    "    df_merged = pd.read_csv('data/merged_insurance_data.csv')\n",
    "    # Convert date columns back to datetime objects as they are read as strings from CSV\n",
    "    date_cols = ['TXN_DATE_TIME', 'POLICY_EFF_DT', 'LOSS_DT', 'REPORT_DT', 'DATE_OF_JOINING']\n",
    "    for col in date_cols:\n",
    "        df_merged[col] = pd.to_datetime(df_merged[col])\n",
    "    print(\"\\n'merged_insurance_data.csv' loaded successfully for feature engineering.\")\n",
    "    print(\"Shape:\", df_merged.shape)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654043c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ce5496",
   "metadata": {},
   "source": [
    "Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ab3195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New features created successfully.\n",
      "   DAYS_POLICY_TO_LOSS  CLAIM_PREMIUM_RATIO  TXN_MONTH\n",
      "0                 1789            57.277413          6\n",
      "1                  753           183.473289          6\n",
      "2                  231            82.676163          6\n",
      "3                 1262            92.555099          6\n",
      "4                 3065            33.886818          6\n"
     ]
    }
   ],
   "source": [
    "#   5a. Time-Based Features  \n",
    "df_merged['DAYS_POLICY_TO_LOSS'] = (df_merged['LOSS_DT'] - df_merged['POLICY_EFF_DT']).dt.days\n",
    "df_merged['DAYS_LOSS_TO_REPORT'] = (df_merged['REPORT_DT'] - df_merged['LOSS_DT']).dt.days\n",
    "df_merged['AGENT_TENURE_AT_REPORT'] = (df_merged['REPORT_DT'] - df_merged['DATE_OF_JOINING']).dt.days\n",
    "df_merged['TXN_MONTH'] = df_merged['TXN_DATE_TIME'].dt.month\n",
    "df_merged['TXN_DAY_OF_WEEK'] = df_merged['TXN_DATE_TIME'].dt.dayofweek\n",
    "\n",
    "#   5b. Interaction and Ratio Features  \n",
    "df_merged['CLAIM_PREMIUM_RATIO'] = df_merged['CLAIM_AMOUNT'] / (df_merged['PREMIUM_AMOUNT'] + 1e-6)\n",
    "df_merged['AGE_AT_POLICY_START'] = df_merged['AGE'] - (df_merged['TXN_DATE_TIME'].dt.year - df_merged['POLICY_EFF_DT'].dt.year)\n",
    "\n",
    "print(\"\\nNew features created successfully.\")\n",
    "print(df_merged[['DAYS_POLICY_TO_LOSS', 'CLAIM_PREMIUM_RATIO', 'TXN_MONTH']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f8034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped 29 unnecessary columns.\n",
      "Shape of DataFrame for modeling: (10000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Drop PII, redundant, and original date columns that are no longer needed for modeling.\n",
    "#   6. Drop PII and Redundant Columns  \n",
    "pii_cols = [\n",
    "    'TRANSACTION_ID', 'CUSTOMER_ID', 'POLICY_NUMBER', 'CUSTOMER_NAME',\n",
    "    'ADDRESS_LINE1', 'ADDRESS_LINE2', 'CITY', 'POSTAL_CODE', 'SSN',\n",
    "    'ROUTING_NUMBER', 'ACCT_NUMBER', 'AGENT_NAME', 'DATE_OF_JOINING',\n",
    "    'ADDRESS_LINE1_emp', 'ADDRESS_LINE2_emp', 'CITY_emp', 'POSTAL_CODE_emp',\n",
    "    'EMP_ROUTING_NUMBER', 'EMP_ACCT_NUMBER', 'VENDOR_NAME',\n",
    "    'ADDRESS_LINE1_vend', 'ADDRESS_LINE2_vend', 'CITY_vend', 'POSTAL_CODE_vend',\n",
    "    'CLAIM_STATUS' # Redundant\n",
    "]\n",
    "date_cols_to_drop = ['TXN_DATE_TIME', 'POLICY_EFF_DT', 'LOSS_DT', 'REPORT_DT']\n",
    "cols_to_drop = pii_cols + date_cols_to_drop\n",
    "\n",
    "df_processed = df_merged.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"\\nDropped {len(cols_to_drop)} unnecessary columns.\")\n",
    "print(\"Shape of DataFrame for modeling:\", df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8667912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values handled.\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "#   7. Handle Missing Values  \n",
    "for col in df_processed.select_dtypes(include=np.number).columns:\n",
    "    if df_processed[col].isnull().any():\n",
    "        median_val = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_val, inplace=True)\n",
    "\n",
    "for col in df_processed.select_dtypes(include=['object', 'category']).columns:\n",
    "    if df_processed[col].isnull().any():\n",
    "        mode_val = df_processed[col].mode()[0]\n",
    "        df_processed[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values handled.\")\n",
    "print(\"Remaining missing values:\", df_processed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4562b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff9730d9",
   "metadata": {},
   "source": [
    "Preprocessing for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c2f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split into training and testing sets.\n",
      "X_train shape: (8000, 31)\n",
      "X_test shape: (2000, 31)\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "# Split the data into training and testing sets before applying scaling and encoding to prevent data leakage.\n",
    "\n",
    "#  8. Train-Test Split \n",
    "X = df_processed.drop('FRAUD_FLAG', axis=1)\n",
    "y = df_processed['FRAUD_FLAG']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\nData split into training and testing sets.\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3e5b",
   "metadata": {},
   "source": [
    "**Preprocessing with Pipelines**\n",
    "\n",
    "Define pipelines to scale numerical data and one-hot encode categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b46685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete.\n",
      "Shape of processed training features: (8000, 2115)\n"
     ]
    }
   ],
   "source": [
    "## 9. Define and Apply Preprocessing\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names for the new DataFrame columns\n",
    "ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "new_column_names = list(numerical_cols) + list(ohe_feature_names)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=new_column_names, index=X_train.index)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=new_column_names, index=X_test.index)\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(\"Shape of processed training features:\", X_train_processed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025b2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771850a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved and ready for modeling in notebook 03.\n"
     ]
    }
   ],
   "source": [
    "#   10. Save Data for Modeling  \n",
    "train_data = pd.concat([X_train_processed_df, y_train], axis=1)\n",
    "test_data = pd.concat([X_test_processed_df, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv('data/train_processed.csv', index=False)\n",
    "test_data.to_csv('data/test_processed.csv', index=False)\n",
    "\n",
    "print(\"\\nProcessed data saved and ready for modeling in notebook 03.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a201f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
